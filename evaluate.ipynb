{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accelerator = Accelerator(cpu=False)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"-------Device:\", accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_floats(float_list):\n",
    "    return [float(f\"{num:.3f}\") for num in float_list]\n",
    "\n",
    "model_sentence_transformers = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v1')\n",
    "model_sentence_transformers = model_sentence_transformers.to(accelerator.device) \n",
    "def compute_metrics(reference_sentence, sentence_to_compare):\n",
    "    embed_pred = model_sentence_transformers.encode([sentence_to_compare], convert_to_tensor=True)\n",
    "    embed_reference = model_sentence_transformers.encode([reference_sentence], convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    sen_trans_score = cos(embed_pred, embed_reference)\n",
    "    cosine_score = tuple(sen_trans_score.detach().cpu().numpy())\n",
    "\n",
    "    rouge = evaluate.load('rouge')\n",
    "    rouge_results = rouge.compute(predictions=[sentence_to_compare], references=[reference_sentence], use_aggregator=False)\n",
    "    rouge_L = rouge_results[\"rougeL\"][0]\n",
    "    rouge_1 = rouge_results[\"rouge1\"][0]\n",
    "    \n",
    "    ref = reference_sentence\n",
    "    comp = sentence_to_compare\n",
    "    length = max(len(ref), len(comp))\n",
    "    ed = (length - Levenshtein.distance(ref, comp)) / length  \n",
    "    ref_set, comp_set = set(ref.split()), set(comp.split())\n",
    "    jac = float(len(ref_set & comp_set)) / len(ref_set | comp_set)\n",
    "        \n",
    "    return format_floats([cosine_score, rouge_1, rouge_L, ed, jac])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(csv_file_path, output_pickle_path, start_idx=0, end_idx=None, checkpoint_interval=500):\n",
    "    # Load the combined CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Determine the end index if not provided\n",
    "    if end_idx is None:\n",
    "        end_idx = len(df)\n",
    "\n",
    "    # Initialize a list to hold all other scores\n",
    "    all_other_scores = []\n",
    "\n",
    "    for idx in tqdm(range(start_idx, end_idx), desc=\"Processing rows: \"):\n",
    "        row_other_scores = []\n",
    "        \n",
    "        reference_para = []\n",
    "        generated_para = []\n",
    "        \n",
    "        for i in range(45): # should be max 45 sentences\n",
    "            sentence_col = f\"Sentence_{i}\"\n",
    "            generated_col = f\"Generated_{i}\"\n",
    "\n",
    "            if sentence_col in df.columns and generated_col in df.columns:\n",
    "                reference_sentence = df.at[idx, sentence_col]\n",
    "                generated_sentence = df.at[idx, generated_col]\n",
    "\n",
    "                if str(reference_sentence) != \"nan\" and str(generated_sentence) != \"nan\":\n",
    "                    reference_para.append(reference_sentence)\n",
    "                    generated_para.append(generated_sentence)\n",
    "\n",
    "                    ref_combined = \"\".join(reference_para)\n",
    "                    gen_combined = \"\".join(generated_para)\n",
    "\n",
    "                    scores = compute_metrics(ref_combined, gen_combined)\n",
    "                    row_other_scores.append(scores)\n",
    "        \n",
    "        all_other_scores.append(row_other_scores)\n",
    "\n",
    "        # Save the checkpoint\n",
    "        if (idx + 1) % checkpoint_interval == 0:\n",
    "            with open(output_pickle_path, 'wb') as checkpoint_file:\n",
    "                pickle.dump(all_other_scores, checkpoint_file)\n",
    "\n",
    "    # Save the final results to the pickle file\n",
    "    with open(output_pickle_path, 'wb') as output_file:\n",
    "        pickle.dump(all_other_scores, output_file)\n",
    "\n",
    "    print(f\"Results saved to {output_pickle_path}\")\n",
    "\n",
    "calculate_scores(\"splitted_paragraph_filtered.csv\", \"scores.pkl\") # TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can run statistical analysis on the scores\n",
    "def read_and_print_pickle(pickle_file_path):\n",
    "    # Load the pickle file\n",
    "    with open(pickle_file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Print the contents of the pickle file\n",
    "    firsts = []\n",
    "    for idx, row in enumerate(data):\n",
    "        firsts.append(row[0])\n",
    "        # print(f\"Row {idx}: {row}\")\n",
    "    print(f\"Percentage of numbers above 0.5: {sum(1 for number in firsts if number > 0.5) / len(firsts) * 100:.2f}%\")\n",
    "    print(f\"Percentage of numbers above 0.9: {sum(1 for number in firsts if number > 0.9) / len(firsts) * 100:.2f}%\")\n",
    "\n",
    "read_and_print_pickle(\"scores.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was used to calculate the table in the paper:\n",
    "def calculate_statistics(data, k):\n",
    "    # Filter data for sentence numbers less than k\n",
    "    filtered_data = [d for d in data if d[\"sentence_number\"] <= k]\n",
    "    \n",
    "    # Initialize dictionaries to store metrics\n",
    "    metrics = {\"COSINE\": [], \"R1\": [], \"RL\": [], \"ED\": [], \"J\": []}\n",
    "    \n",
    "    # Populate the metrics dictionaries\n",
    "    for d in filtered_data:\n",
    "        for metric in metrics:\n",
    "            metrics[metric].append(d[metric])\n",
    "    \n",
    "    # Calculate mean, std, and percentage of scores above 0.9 and 1\n",
    "    statistics = {}\n",
    "    if k != 45:\n",
    "        statistics[\"sentence_number\"] = k\n",
    "    else:\n",
    "        statistics[\"sentence_number\"] = \"ALL\"\n",
    "    for metric, values in metrics.items():\n",
    "        if values:  # Check if there are values to avoid division by zero\n",
    "            mean_value = np.mean(values)\n",
    "            std_value = np.std(values)\n",
    "            pct_above_0_9 = np.sum(np.array(values) >= 0.9) / len(values) * 100\n",
    "            pct_above_1 = np.sum(np.array(values) >= 1) / len(values) * 100\n",
    "        else:\n",
    "            mean_value = std_value = pct_above_0_9 = pct_above_1 = 0\n",
    "        if metric == \"COSINE\":\n",
    "            pct_above_05 = np.sum(np.array(values) >= 0.5) / len(values) * 100\n",
    "            statistics[metric] = {\n",
    "                f'{metric} mean +- std' : fr'$ {mean_value:.2f} \\pm {std_value:.2f} $',\n",
    "                f\"{metric} >= 0.9\": pct_above_0_9,\n",
    "                f\"{metric} >= 1.0\": pct_above_1, \n",
    "                f\"{metric} >= 0.5\": pct_above_05\n",
    "            }\n",
    "        else:\n",
    "            statistics[metric] = {\n",
    "                f'{metric} mean +- std' : fr'$ {mean_value:.2f} \\pm {std_value:.2f} $',\n",
    "                f\"{metric} >= 0.9\": pct_above_0_9,\n",
    "                f\"{metric} >= 1.0\": pct_above_1\n",
    "            }\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "def make_dict(Index, sentenceNumber, cosine, R1, RL, ED, J):\n",
    "    return {\"Paragraph\": Index, \"sentence_number\": sentenceNumber, \"COSINE\": cosine, \"R1\": R1, \"RL\": RL, \"ED\": ED, \"J\": J}\n",
    "\n",
    "scores = pickle.load(open(\"scores.pkl\", \"rb\"))\n",
    "total_scores = []\n",
    "for index in range(len(scores)):\n",
    "    for sentenceNumber in range(len(scores[index])):\n",
    "        total_scores.append(make_dict(index, sentenceNumber + 1, scores[index][sentenceNumber][0], scores[index][sentenceNumber][1], \n",
    "                                      scores[index][sentenceNumber][2], scores[index][sentenceNumber][3], scores[index][sentenceNumber][4]))\n",
    "rows = []\n",
    "for k in [1,2,3,5,10,20,45]:\n",
    "    rows.append(calculate_statistics(total_scores, k))\n",
    "\n",
    "print(rows)\n",
    "table_df = pd.DataFrame(rows)\n",
    "table_df.to_csv(\"table_for_paper_all_scores.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
